# 目录

- [1.目前主流的AI视频技术框架有哪几种？](#1.目前主流的AI视频技术框架有哪几种？)
- [2.目前主流的AI视频大模型有哪些？](#2.目前主流的AI视频大模型有哪些？)
- [3.Sora有哪些创新点？](#3.Sora有哪些创新点？)
- [4.SVD（Stable-Video-Diffusion）有哪些创新点？](#4.SVD（Stable-Video-Diffusion）有哪些创新点？)
- [5.AIGC时代的主流AI视频生成流程有哪些？](#5.AIGC时代的主流AI视频生成流程有哪些？)
- [6.SAM v2] (#6.SAM v2)


<h2 id="1.目前主流的AI视频技术框架有哪几种？">1.目前主流的AI视频技术框架有哪几种？</h2>

Rocky梳理总结了AIGC时代到目前为止主流的AI视频技术框架，市面上的所有AI视频产品基本上都是基于以下这些框架：
1. 文本生成视频：输入文本，先生成图片或者直接生成视频。主要流程包括工作流前处理+扩散模型+运动模块+条件控制+工作流后处理。
2. 图像生成视频：输入图像，先生成前后帧图像，然后使用插帧与语义扩展持续生成前后序列帧图像，最后生成完整视频。主要流程包括工作流前处理+扩散模型+运动模块+条件控制+工作流后处理。
3. 视频生成视频：输入视频，提取关键帧，对关键帧进行转绘，然后再进行插帧，从而生成新的视频。主要流程包括工作流前处理+扩散模型+运动模块+条件控制+工作流后处理。


<h2 id="2.目前主流的AI视频大模型有哪些？">2.目前主流的AI视频大模型有哪些？</h2>

Rocky为大家梳理总结了AIGC时代到目前为主的主流AI视频大模型，如下所示：

1. Stable Video Diffusion（SVD）系列
2. Sora
3. 可灵AI
4. LUMA
5. Gen系列
6. Stable Diffusion系列 + Animatediff


<h2 id="3.Sora有哪些创新点？">3.Sora有哪些创新点？</h2>

OpenAI对Sora的定位不只是视频生成工具，而是希望在此基础上开发出能够让计算机理解真实世界的算法与技术——“作为世界模拟器的视频生成模型”。在这个宏大愿景下最具潜力的技术基底之一便是生成式模型 (generative model)。

下面是Sora的一些创新点：
1. 海量的数据：在Sora的技术报告中，关于数据量级是一句话都没有提。这就说明，Sora使用了海量的高质量视频数据用作训练，Rocky相信未来全互联网的视频数据都会被Sora用作训练，同时在视频数据领域的数据生成、数据增强将会有非常大的机会。
2. 灵活编码：在Sora中，借鉴了大语言模型的构建方式，使用video compression network（convolutional VAEs）将视频数据tokenizer化，获得visual patches，使得任何长度和内容的视频都能编码成AI视频模型可以直接处理（输入/输出）的embeddings。首先video compression network将输入视频的时间和空间两个维度同时进行压缩，编码成一个和视频大小成正比的3D visual patch矩阵，然后再将其展开成1D array of patches Embeddings，送入到后续的DiT model中。这样可以带来两个好处，分别是让Sora能够生成不同分辨率的视频分和生成的视频的边框更加合理。
3. DiT模型架构：Sora使用了DiT（Diffusion Transformer）作为核心架构，这让Transformer在AI领域的大一统更进一步。
4. 精细化数据标注：和DALL-E 3一样，OpenAI用内部标注工具（可能是GPT4-4o等）给视频数据进行详尽的描述标注，从而提升Sora模型生成视频与输入prompt的一致性、生成视频的质量和视频中正确显示文本的能力。Rocky认为数据工程是非常关键的一点，无论是传统深度学习时代还是AIGC时代，都是AI领域的杀手锏。
5. 让AI视频领域的Scaling Law成立：保证模型越大，数据越多，效果就越好。Sora也不例外。一句话概括Sora的贡献，便是：在足量的数据，优质的标注，灵活的编码下，scaling law 在 transformer + diffusion model 的架构上继续成立。


<h2 id="4.SVD（Stable-Video-Diffusion）有哪些创新点？">4.SVD（Stable-Video-Diffusion）有哪些创新点？</h2>

Rocky认为SVD（Stable Video Diffusion）模型非常有价值，其开源精神让我们动容，下面是SVD模型的主要创新点：
1. 基于Stable Diffusion 2.1模型架构
2. 海量数据集：StabilityAI使用了一个包含5.8亿个视频剪辑的巨大数据集，来训练SVD模型。为了筛选高质量数据，首先需要检测每个视频中的不同镜头和转场，并且评估每个镜头中的运动信息，然后为每个镜头自动生成描述文字和每个镜头的美学效果等。
3. 数据精细化处理：（1）级联切换检测：采用级联的切换检测方法识别视频中的场景转场。（2）运动信息提取:基于稠密光流估计每个视频片段的运动信息。（3）文本描述生成:为每个视频片段自动生成三种形式的文字描述。（4）质量评估:使用CLIP等方法评估每个片段的视觉质量、文本匹配度等。（5）过滤去噪:根据上述评估指标过滤掉质量较差的视频片段。经过层层筛选，最后保留了一个约1.5亿视频片段的超高质量数据集，为后续的SVD模型训练奠定重要基础。
4. 多阶段训练：SVD模型在模型训练方面也与传统方法不同，其采用了一个三层训练架构。第一阶段是进行图像预训练，初始化一个图像生成模型。第二阶段是在已经构建的大规模视频数据集上进行视频预训练，学习运动表征。第三阶段是在一个小规模的高质量视频数据集上进行微调。这种分阶段的训练策略可以让模型更好地生成高保真视频。


<h2 id="5.AIGC时代的主流AI视频生成流程有哪些？">5.AIGC时代的主流AI视频生成流程有哪些？</h2>

Rocky总结了如下图所示的AIGC时代主流AI视频生成流程，可以作为大家构建AI视频产品构架的基础底座：

![AIGC时代的主流AI视频生成流程](./imgs/AIGC时代的主流AI视频生成流程.jpg)

## 6.SAM v2

